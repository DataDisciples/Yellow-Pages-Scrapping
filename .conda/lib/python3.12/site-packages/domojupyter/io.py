from typing import Any, Optional

import numpy as np

import domojupyter.ai as ai
from .domo import Domo, DomoDataflowAuthentication, DomoHubAuthentication, DomoResourceTokenAuthentication
from .jupyterhub import Jupyterhub
from io import StringIO
from dateutil.parser import parse
import pandas as pd
import os

__all__ = ['read_dataframe', 'write_dataframe', 'get_account_property_value', 'get_account_property_keys']

jupyterhub_services_url = os.getenv('JUPYTERHUB_SERVICES_URL', None)
jupyterhub_api_token = os.getenv('JUPYTERHUB_API_TOKEN', None)
domo_hostname = os.getenv('DOMO_HOSTNAME', None)
domo_resource_token = os.getenv('DOMO_RESOURCE_TOKEN')
workspace_id = os.getenv('DOMO_WORKSPACE_ID', None)
workspace_session_id = os.getenv('DOMO_WORKSPACE_SESSION_ID', None)

if domo_resource_token is None:
    _jupyterhub = Jupyterhub(jupyterhub_services_url, jupyterhub_api_token, workspace_id, workspace_session_id)
    _domo = Domo(domo_hostname, DomoHubAuthentication(_jupyterhub), workspace_id)
else:
    _resource_token_domo = Domo(domo_hostname, DomoResourceTokenAuthentication(domo_resource_token), workspace_id)
    _domo = Domo(domo_hostname, DomoDataflowAuthentication(_resource_token_domo, workspace_id), workspace_id)


def _find_input_datasource_id(datasource):
    workspace = _domo.get_workspace()
    for input in workspace['inputConfiguration']:
        if input['dataSourceId'] == datasource:
            return datasource

    for input in workspace['inputConfiguration']:
        if input['alias'] == datasource:
            return input['dataSourceId']

    return None


def _find_output_datasource_id(datasource):
    workspace = _domo.get_workspace()
    for output in workspace['outputConfiguration']:
        if output['dataSourceId'] == datasource:
            return datasource

    for output in workspace['outputConfiguration']:
        if output['alias'] == datasource:
            return output['dataSourceId']

    return None


def get_schema_from_datasource(datasource):
    datasource_id = _find_input_datasource_id(datasource)

    if datasource_id is None:
        raise Exception(f'Input dataset with id or alias {datasource} missing or not configured for workspace.')
    return {'datasource_id': datasource_id, 'schema': _domo.get_schema(datasource_id).get('columns')}


def get_schema_from_dataframe(dataframe: pd.DataFrame) -> Optional[list[dict[str, Any]]]:
    if dataframe.empty:
        return None
    if not hasattr(dataframe, 'name'):
        print('Dataframe had no provided name, you can set the name by calling dataframe.name = \'name\'')
        dataframe.name = None
    schema = dataframe.dtypes.reset_index()
    schema.columns = ['name', 'type']
    schema['type'] = schema['type'].map(lambda x: np.dtype('int64').name if x == 'Int64' else np.dtype(x).name)
    schema_dict = schema.to_dict(orient='records')

    return schema_dict


def _find_account_id(account_identifier):
    workspace = _domo.get_workspace()
    for account in workspace['accountConfiguration']:
        if account['account_id'] == account_identifier:
            return account['account_id']
        elif account['alias'] == account_identifier:
            return account['account_id']

    return None


def _convert_type(df_type):
    if df_type == 'object':
        return 'STRING'
    elif df_type == 'category':
        return 'STRING'
    elif df_type == 'int64':
        return 'LONG'
    elif df_type == 'Int64':
        return 'LONG'
    elif df_type == 'float64':
        return 'DOUBLE'
    elif df_type == 'bool':
        return 'LONG'
    elif df_type == 'datetime64' or df_type == 'datetime64[ns]':
        return 'DATETIME'
    return 'STRING'


def _convert_type_read(schema_type):
    if schema_type == 'STRING':
        return 'object'
    elif schema_type == 'LONG':
        return 'Int64'
    elif schema_type == 'DOUBLE':
        return 'float64'
    elif schema_type == 'DATETIME':
        return 'datetime64'
    elif schema_type == 'DATE':
        return 'datetime64'
    elif schema_type == 'DECIMAL':
        return 'float64'
    else:
        return 'object'


def _get_schema_definition(df):
    columns = [{'name': column, 'type': _convert_type(df[column].dtype.name)} for column in df.columns]
    return {'columns': columns}


def read_dataframe(dataset, query=None, nlq=None, **kwargs):
    """
    Read Domo dataset into Pandas dataframe

    Parameters:
        dataset (str): Input dataset alias or id
        query (str): Optional SQL-like query string. e.g. 'SELECT col1, col2 FROM table LIMIT 1000'
        nlq (str): Optional NLQ query string. e.g. 'Show me sales by month'

    Returns:
        DataFrame: Pandas dataframe containing dataset or query result set.
    """
    if query is not None and nlq is not None:
        print('Both query and nlq parameters can\'t be provided, NLQ will be ignored.')
    elif nlq is not None:
        query = ai.text_to_sql(nlq, workspace_data_source_alias=dataset).choices[0]['output']
        print(f'Executing NLQ generated query: {query}')
    schema = get_schema_from_datasource(dataset)
    data = _domo.export_datasource(schema.get('datasource_id'), query)

    if not schema:
        raise Exception(f'Returned schema from Mako is empty for datasourceId: {id}.')

    column_types = []
    date_cols = []
    type_dict = {}
    header = pd.read_csv(StringIO(data), index_col=False, header=0, nrows=0).columns.tolist()
    user_dtype = False
    user_parse_date = False

    for cols in schema.get('schema'):
        column_types.append(list(cols.values()))

    if kwargs:
        for key, value in kwargs.items():
            if key == 'dtype':
                user_dtype = True
            elif key == 'parse_dates':
                user_parse_date = True

    for cols in column_types:
        column_name = cols[0]
        if column_name in header:
            dtype = _convert_type_read(cols[1])
            if dtype == 'datetime64' and not user_parse_date:
                date_cols.append(column_name)
            elif not user_dtype:
                type_dict[column_name] = dtype

    if date_cols:
        kwargs['parse_dates'] = date_cols

    if type_dict:
        kwargs['dtype'] = type_dict

    return pd.read_csv(StringIO(data), **kwargs)


def write_dataframe(df, dataset, update_method=None, update_key=None, partition_name=None):
    """
    Write Pandas dataframe to Domo dataset 

    Parameters:
        df: passed in dataframe
        dataset (str): Output dataset alias or id
        update_method (str): Optional method name EX: upsert/append/partition
        update_key (str): Optional column key used for selecting which columns to upsert on
        partition_name (str): Optional tag/id that's required for the partition upload
    """
    datasource_id = _find_output_datasource_id(dataset)

    if datasource_id is None:
        raise Exception(f'Output datasource with id or alias {dataset} missing or not configured for workspace.')

    schema = _get_schema_definition(df)
    existing = _domo.get_schema(datasource_id)

    new_columns = list(tuple(d.items()) for d in schema.get('columns', []))
    duplicate_columns = {t for t in new_columns if new_columns.count(t) > 1}

    if duplicate_columns:
        raise Exception(f'Duplicate column name(s) {[x[0][1] for x in duplicate_columns]} found in schema definition.')

    if update_method is not None:
        update_method = update_method.upper()

    is_upsert = update_method == "UPSERT"
    is_append = update_method == "APPEND"
    is_partition = update_method == "PARTITION"
    is_replace = update_method == "REPLACE"

    if update_method is not None and not is_upsert and not is_append and not is_partition and not is_replace:
        raise Exception(
            f'Provided update method was not a valid value (UPSERT, APPEND, PARTITION, REPLACE), update method: {update_method}.')

    if schema.get('columns', []) != existing.get('columns', []):
        if (is_upsert or is_partition or is_append) and existing.get('columns') is not None:
            raise Exception(
                f'New data schema does not match current schema, which is required for update methods UPSERT, PARTITION or APPEND - provided update method is: {update_method}.')
        _domo.update_schema(datasource_id, schema)
    elif is_upsert:
        existing.update(objects=[{"Identity": update_key}])
        _domo.update_schema(datasource_id, existing)
    elif (is_partition or is_append or is_replace) and existing.get('objects') is not None:
        existing.update(objects=None)
        _domo.update_schema(datasource_id, existing)

    start_execution = _domo.create_execution(datasource_id, update_method)
    execution_id = start_execution.get('executionId', None)

    part = 0
    rowcount = len(df.index)
    chunksize = 50000
    while part * chunksize < rowcount:
        index = part * chunksize
        slice = df[index:index + chunksize]
        _domo.upload_part(datasource_id, execution_id,
                          part, slice.to_csv(header=False, index=False))
        part += 1

    if is_partition:
        if partition_name is None:
            raise Exception(f'Update method was set to PARTITION, but the required update_key was not provided.')
        _domo.commit_execution(datasource_id, execution_id, partition_name)
    else:
        _domo.commit_execution(datasource_id, execution_id, None)


def get_account_property_value(account, key):
    account_id = _find_account_id(account)
    properties = _domo.get_account_properties(account_id)
    if properties is None:
        raise Exception(f'Account properties were not found for accountId {account_id}')
    value = properties.get(key, None)
    if value is None:
        raise Exception(f'Account property values were not found for accountId {account_id} and key: {key}')
    return value


def get_account_property_keys(account):
    account_id = _find_account_id(account)
    properties = _domo.get_account_properties(account_id)
    if properties is None:
        raise Exception(f'Account properties were not found for accountId {account_id}')
    keys = list(properties.keys())
    if keys is None:
        raise Exception(f'Account property values were not found for accountId {account_id}')
    return keys
