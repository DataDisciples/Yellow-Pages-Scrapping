get_ipython().getoutput("pip install scrapy")
get_ipython().getoutput("pip install pandas")


get_ipython().getoutput("pip install nest_asyncio")


pip install scrapy-user-agents


get_ipython().getoutput("pip install scrapy")


get_ipython().getoutput("scrapy runspider yellow_pages_spider.py -o Marion_IL_businesses.csv")


get_ipython().getoutput("scrapy runspider yellow_pages_spider.py -a cities="Carbondale,Mt. Vernon,Murphysboro,Harrisburg,O'Fallon,Herrin,Edwardsville,Nashville,Pinckneyville,West Frankfort,Metropolis"")


with open("yellow_pages_spider.py", "w") as f:
    f.write("""
import scrapy
import os

class YellowPagesSpider(scrapy.Spider):
    name = 'yellowpages'
    start_urls = []

    def __init__(self, cities=None, *args, **kwargs):
        super(YellowPagesSpider, self).__init__(*args, **kwargs)
        if cities:
            for city in cities.split(","):
                city_url = f"https://www.yellowpages.com/search?search_terms=all&geo_location_terms={city.replace(' ', '%20')},IL"
                self.start_urls.append(city_url)
        else:
            self.start_urls = ['https://www.yellowpages.com/search?search_terms=all&geo_location_terms=Marion,IL']

    def parse(self, response):
        city = response.url.split('geo_location_terms=')[1].split(',IL')[0].replace('%20', '_')
        filename = f"{city}_IL_businesses.csv"
        self.custom_settings = {
            'FEED_URI': filename,
            'FEED_FORMAT': 'csv'
        }

        for business in response.css('div.v-card'):
            yield {
                'name': business.css('a.business-name span::text').get(default=''),
                'address': business.css('div.info-section div.street-address::text').get(default=''),
                'phone': business.css('div.info-section div.phones::text').get(default=''),
                'category': business.css('div.info div.categories a::text').get(default=''),
                'website': business.css('a.track-visit-website::attr(href)').get(default='')
            }

        next_page = response.css('a.next.ajax-page::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
    """)

print("yellow_pages_spider.py file created successfully.")


get_ipython().getoutput("scrapy runspider yellow_pages_spider.py -a cities="Carbondale,Mt. Vernon,Murphysboro,Harrisburg,O'Fallon,Herrin,Edwardsville,Nashville,Pinckneyville,West Frankfort,Metropolis"")



import os


import pandas as pd

cities = [
    'Carbondale', 'Mt. Vernon', 'Murphysboro', 'Harrisburg', 
    "O'Fallon", 'Herrin', 'Edwardsville', 'Nashville', 
    'Pinckneyville', 'West Frankfort', 'Metropolis'
]

# Function to load and display data from each city's CSV file
def load_and_display(city):
    city_file = f"{city.replace(' ', '_')}_IL_businesses.csv"
    if os.path.exists(city_file):
        df = pd.read_csv(city_file)
        print(f"\n{city} Businesses")
        print(df.head())
        print(f"Total number of businesses pulled from {city}: {len(df)}")
    else:
        print(f"No data found for {city}")

for city in cities:
    load_and_display(city)


with open("yellow_pages_spider.py", "w") as f:
    f.write("""
import scrapy

class YellowPagesSpider(scrapy.Spider):
    name = 'yellowpages'
    start_urls = []

    def __init__(self, cities=None, *args, **kwargs):
        super(YellowPagesSpider, self).__init__(*args, **kwargs)
        if cities:
            for city in cities.split(","):
                city_url = f"https://www.yellowpages.com/search?search_terms=all&geo_location_terms={city.replace(' ', '%20')},IL"
                self.start_urls.append(city_url)
        else:
            self.start_urls = ['https://www.yellowpages.com/search?search_terms=all&geo_location_terms=Marion,IL']

    custom_settings = {
        'FEED_URI': 'Southern_IL_businesses.csv',
        'FEED_FORMAT': 'csv',
        'FEED_EXPORT_FIELDS': ['name', 'address', 'phone', 'category', 'website']
    }

    def parse(self, response):
        for business in response.css('div.v-card'):
            yield {
                'name': business.css('a.business-name span::text').get(default=''),
                'address': business.css('div.info-section div.street-address::text').get(default=''),
                'phone': business.css('div.info-section div.phones::text').get(default=''),
                'category': business.css('div.info div.categories a::text').get(default=''),
                'website': business.css('a.track-visit-website::attr(href)').get(default='')
            }

        next_page = response.css('a.next.ajax-page::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
    """)

print("yellow_pages_spider.py file created successfully.")


get_ipython().getoutput("scrapy runspider yellow_pages_spider.py -a cities="Carbondale,Mt. Vernon,Murphysboro,Harrisburg,O'Fallon,Herrin,Edwardsville,Nashville,Pinckneyville,West Frankfort,Metropolis"")



import pandas as pd

# Load the CSV file
df = pd.read_csv('Southern_IL_businesses.csv')

# Display the first few rows of the DataFrame
print(df.head())

# Check the number of entries
print(f"Total number of businesses pulled: {len(df)}")


get_ipython().run_cell_magic("writefile", " more-cities.py", """import scrapy

class YellowPagesSpider(scrapy.Spider):
    name = 'yellow_pages'
    allowed_domains = ['yellowpages.com']

    def start_requests(self):
        cities = [
            'Sesser', 'Royalton', 'Cairo', 'Vienna', 'Zeigler',
            'Stonefort', 'Thompsonville', 'Johnston City', 'Eldorado',
            'Benton', 'Galatia', 'Mulkeytown', 'Ozark', 'Akin', 'Christopher'
        ]
        for city in cities:
            url = f"https://www.yellowpages.com/search?search_terms=all&geo_location_terms={city},IL"
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        for business in response.css('div.result'):
            yield {
                'name': business.css('a.business-name span::text').get(),
                'address': business.css('div.street-address::text').get(),
                'phone': business.css('div.phones.phone.primary::text').get(),
                'category': business.css('div.categories a::text').get(),
                'website': business.css('a.track-visit-website::attr(href)').get(default=''),
            }
        
        next_page = response.css('a.next::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)""")


import pandas as pd

# Load the existing CSV file
existing_df = pd.read_csv('Southern_IL_businesses.csv')

# Load the new data
new_df = pd.read_csv('new_Southern_IL_businesses.csv')

# Append the new data to the existing data
combined_df = pd.concat([existing_df, new_df])

# Save the combined data back to the CSV file
combined_df.to_csv('Southern_IL_businesses.csv', index=False)

# Display the total number of entries
print(f"Total number of businesses pulled: {len(combined_df)}")


import pandas as pd

# Load the combined CSV file
final_df = pd.read_csv('Southern_IL_businesses.csv')

# Display the first few rows of the DataFrame
print(final_df.head())

# Display the total number of entries
print(f"Total number of businesses pulled: {len(final_df)}")


import pandas as pd

# Load the St. Louis Metropolitan businesses data
st_louis_df = pd.read_csv('st_louis_metropolitan_businesses.csv')

# Display the first few rows of the DataFrame
print(st_louis_df.head())

# Display the total number of entries
print(f"Total number of businesses pulled: {len(st_louis_df)}")


get_ipython().getoutput("scrapy runspider st_louis_spider.py -o st_louis_metropolitan_businesses.csv")


import os

def create_spider_script(cities, filename):
    spider_script = f"""
import scrapy

class {filename.split('.')[0]}(scrapy.Spider):
    name = '{filename.split('.')[0]}'
    start_urls = {cities}

    def parse(self, response):
        for business in response.css('div.result'):
            yield {{
                'name': business.css('a.business-name span::text').get(),
                'address': business.css('div.street-address::text').get(),
                'phone': business.css('div.phones.phone.primary::text').get(),
                'category': business.css('div.categories a::text').get(),
                'website': business.css('div.links a::attr(href)').get()
            }}

        next_page = response.css('a.next.ajax-page::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
"""

    with open(filename, 'w') as file:
        file.write(spider_script)

    print(f"Spider script '{filename}' has been created.")

# List of cities
cities = [
    'Ballwin,MO', 'Brentwood,MO', 'Chesterfield,MO', 'Clayton,MO', 'Creve Coeur,MO',
    'Des Peres,MO', 'Ellisville,MO', 'Fenton,MO', 'Florissant,MO', 'Hazelwood,MO',
    'Kirkwood,MO', 'Ladue,MO', 'Manchester,MO', 'Maplewood,MO', 'Maryland Heights,MO',
    'Mehlville,MO', 'Olivette,MO', "O Fallon,MO", 'Richmond Heights,MO', 'Saint Charles,MO',
    'Saint Peters,MO', 'Shrewsbury,MO', 'Town and Country,MO', 'University City,MO', 
    'Webster Groves,MO', 'Saint Louis,MO'
]

# Split the cities into smaller groups
groups = [cities[i:i + 5] for i in range(0, len(cities), 5)]

# Create spider scripts for each group
for i, group in enumerate(groups):
    start_urls = [f'https://www.yellowpages.com/search?search_terms=all&geo_location_terms={city}' for city in group]
    filename = f'st_louis_spider_{i}.py'
    create_spider_script(start_urls, filename)


import pandas as pd
import os

output_file = 'st_louis_metropolitan_businesses.csv'

# Function to run a spider script and append the results to the main output file
def run_spider_and_append(spider_filename, temp_output_file):
    os.system(f"scrapy runspider {spider_filename} -o {temp_output_file} -t csv")
    
    # Append to the main output file
    if os.path.exists(output_file):
        temp_df = pd.read_csv(temp_output_file)
        temp_df.to_csv(output_file, mode='a', header=False, index=False)
    else:
        os.rename(temp_output_file, output_file)
    
    # Clean up temporary files
    if os.path.exists(temp_output_file):
        os.remove(temp_output_file)

# Run each spider script separately
for i in range(len(groups)):
    spider_filename = f'st_louis_spider_{i}.py'
    temp_output_file = f'temp_output_{i}.csv'
    run_spider_and_append(spider_filename, temp_output_file)



